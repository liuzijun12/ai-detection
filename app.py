# æ ‡å‡†åº“
import os, re, json, time, datetime, threading, logging, webbrowser

# å›¾åƒä¸æ¨¡å‹
from PIL import Image
import torch
import torchvision.transforms as transforms
from torch import nn
from efficientnet_pytorch import EfficientNet

# Flask ä¸æ‰©å±•
from flask import Flask, request, jsonify, send_from_directory, session
from flask_cors import CORS
from flask_sqlalchemy import SQLAlchemy
from functools import wraps
from werkzeug.utils import secure_filename  # å¯ç•™ä½œåç»­ä¸Šä¼ 

# AI ä¸ NLP
import requests
from openai import OpenAI

# è‡ªå®šä¹‰æ¨¡å—
from models import db, User, Case

# Word æ–‡æ¡£ç”Ÿæˆ
from docx import Document
from docx.enum.text import WD_ALIGN_PARAGRAPH

import base64
import uuid
from datetime import timedelta

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# å…¨å±€é…ç½®
MODEL_PATH = "best_model.pth"  # è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹
IMAGE_SIZE = 320               # è®­ç»ƒæ—¶çš„å›¾åƒå°ºå¯¸
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å¤§è¯­è¨€æ¨¡å‹é…ç½®
LLM_CONFIG = {
    "model_type": "local",  # local æˆ– api
    "model_name": "deepseek-r1:1.5b",  # é»˜è®¤æ¨¡å‹åç§°
    "api_url": "",  # API URLï¼Œå¦‚æœä½¿ç”¨API
    "api_key": "",  # APIå¯†é’¥ï¼Œå¦‚æœä½¿ç”¨API
}

# æ·»åŠ å›¾ç‰‡ä¿å­˜é…ç½®
UPLOAD_FOLDER = os.path.join('ai_detection', 'staticImage')
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def save_base64_image(base64_string, folder):
    """ä¿å­˜Base64å›¾ç‰‡å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    try:
        # ä»Base64å­—ç¬¦ä¸²ä¸­æå–å®é™…çš„å›¾ç‰‡æ•°æ®
        if ',' in base64_string:
            base64_string = base64_string.split(',')[1]
        
        # è§£ç Base64æ•°æ®
        image_data = base64.b64decode(base64_string)
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        filename = f"{uuid.uuid4().hex}.png"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(folder, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        filepath = os.path.join(folder, filename)
        with open(filepath, 'wb') as f:
            f.write(image_data)
        
        return filepath
    except Exception as e:
        print(f"Error saving base64 image: {e}")
        return None

def input_with_timeout(prompt, timeout=10, default="2"):
    """
    è·¨å¹³å°å®ç°ç”¨æˆ·è¾“å…¥è¶…æ—¶ï¼Œé»˜è®¤é€‰æ‹© defaultã€‚
    Windows + Linux é€šç”¨ã€‚
    """
    result = {"value": default}

    def get_input():
        try:
            user_input = input(prompt)
            if user_input.strip() in ["1", "2"]:
                result["value"] = user_input.strip()
        except Exception:
            pass

    thread = threading.Thread(target=get_input)
    thread.daemon = True
    thread.start()
    thread.join(timeout)

    if result["value"] == default:
        print(f"\næœªåœ¨ {timeout} ç§’å†…è¾“å…¥ï¼Œé»˜è®¤é€‰æ‹© {default}ï¼ˆç½‘ç»œAPIï¼‰")
    return result["value"]




def setup_llm_config():
    """è®¾ç½®å¤§è¯­è¨€æ¨¡å‹é…ç½®"""
    print("\n" + "="*50)
    print("æ¬¢è¿ä½¿ç”¨çœ¼éƒ¨ç–¾ç—…é¢„æµ‹ä¸­å¿ƒ")
    print("="*50)

    print("\nè¯·é€‰æ‹©AIé—®è¯Šä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ç±»å‹:")
    print("1. æœ¬åœ°æ¨¡å‹ (Ollama)")
    print("2. ç½‘ç»œAPI (å¦‚OpenAI, Azureç­‰)")

    choice = input_with_timeout("\nè¯·è¾“å…¥é€‰é¡¹ (1/2): ", timeout=10, default="2")

    if choice == "1":
        LLM_CONFIG["model_type"] = "local"

        # è·å–å¯ç”¨çš„æœ¬åœ°æ¨¡å‹åˆ—è¡¨
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if models:
                    print("\nå¯ç”¨çš„æœ¬åœ°æ¨¡å‹:")
                    for i, model in enumerate(models, 1):
                        print(f"{i}. {model.get('name')}")

                    model_choice = ""
                    while not model_choice.isdigit() or int(model_choice) < 1 or int(model_choice) > len(models):
                        model_choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}): ").strip()

                    selected_model = models[int(model_choice)-1].get('name')
                    LLM_CONFIG["model_name"] = selected_model
                    print(f"\nå·²é€‰æ‹©æ¨¡å‹: {selected_model}")
                else:
                    print("\næœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹: deepseek-r1:1.5b")
                    print("å¦‚éœ€å®‰è£…æ¨¡å‹ï¼Œè¯·è¿è¡Œ: ollama pull deepseek-r1:1.5b")
            else:
                print("\nOllamaæœåŠ¡æœªå“åº”ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
                print("å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹: deepseek-r1:1.5b")
        except Exception as e:
            print(f"\nè·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            print("å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹: deepseek-r1:1.5b")
            print("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œæˆ–è€…è¿è¡Œ: ollama pull deepseek-r1:1.5b")

    else:  # choice == "2"
        LLM_CONFIG["model_type"] = "api"

        # api_url = input("\nè¯·è¾“å…¥API URL (ä¾‹å¦‚: https://api.openai.com/v1/chat/completions): ").strip()
        api_url = "https://api.deepseek.com"
        # https: // api.deepseek.com / v1
        # api_key = input("è¯·è¾“å…¥APIå¯†é’¥: ").strip()
        api_key = "sk-aa19071022fe47f1aaef8a0215749ddc"
        # model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: gpt-3.5-turbo): ").strip()
        model_name = "deepseek-chat"

        LLM_CONFIG["api_url"] = api_url
        LLM_CONFIG["api_key"] = api_key

        if model_name:
            LLM_CONFIG["model_name"] = model_name

    print("\né…ç½®å®Œæˆï¼æ­£åœ¨å¯åŠ¨æœåŠ¡å™¨...\n")
    return LLM_CONFIG

# åŠ è½½æ¨¡å‹
def load_model(model_path):
    try:
        print("Loading model...")
        model = EfficientNet.from_name("efficientnet-b3")
        num_classes = 8
        model._fc = nn.Linear(model._fc.in_features, num_classes)

        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=DEVICE)
            model.load_state_dict(checkpoint["model_state_dict"])
            model.eval().to(DEVICE)
            print("Model loaded successfully!")
            logger.info("Model loaded successfully")
        else:
            print(f"Warning: Model file {model_path} not found!")
            return None
        return model
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        logger.error("Error loading model: %s", str(e))
        return None

# é¢„å¤„ç†å‡½æ•°
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0).to(DEVICE)

app = Flask(__name__, static_folder='ai_detection')
CORS(app, supports_credentials=True)  # æ”¯æŒè·¨åŸŸè¯·æ±‚
app.secret_key = 'your-secret-key'  # æ›´æ¢ä¸ºéšæœºçš„å¯†é’¥


# é…ç½® MySQL æ•°æ®åº“è¿æ¥ï¼ˆè¿æ¥ä½ å®¹å™¨é‡Œçš„ eye_dbï¼‰
app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://root:123456@localhost:3306/eye_db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# åˆå§‹åŒ–æ•°æ®åº“å¯¹è±¡
db.init_app(app)

@app.route("/register", methods=["POST"])
def register():
    data = request.get_json()
    username = data.get("username")
    password = data.get("password")

    if not username or not password:
        return jsonify({"success": False, "message": "ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º"}), 400

    # æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦å­˜åœ¨
    if User.query.filter_by(username=username).first():
        return jsonify({"success": False, "message": "ç”¨æˆ·åå·²å­˜åœ¨"}), 409

    # åˆ›å»ºç”¨æˆ·
    new_user = User(username=username, password=password)
    db.session.add(new_user)
    db.session.commit()

    return jsonify({"success": True, "message": "æ³¨å†ŒæˆåŠŸ"})

@app.route("/login", methods=["POST"])
def login():
    data = request.get_json()
    username = data.get("username")
    password = data.get("password")

    if not username or not password:
        return jsonify({"success": False, "message": "ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º"}), 400

    user = User.query.filter_by(username=username).first()
    if not user:
        return jsonify({"success": False, "message": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

    if user.password != password:
        return jsonify({"success": False, "message": "å¯†ç é”™è¯¯"}), 401

    # è®¾ç½®session
    session['logged_in'] = True
    session['username'] = username
    session['user_id'] = user.id
    session.permanent = True  # è®¾ç½®sessionæŒä¹…åŒ–

    return jsonify({
        "success": True,
        "message": "ç™»å½•æˆåŠŸ",
        "username": username
    })

# æ·»åŠ sessionæ£€æŸ¥æ¥å£
@app.route("/check_session")
def check_session():
    return jsonify({
        "logged_in": session.get('logged_in', False),
        "username": session.get('username'),
        "user_id": session.get('user_id')
    })

# ä¿®æ”¹appé…ç½®
def configure_app(app):
    # Sessioné…ç½®
    app.config.update(
        SESSION_COOKIE_SECURE=False,  # å¼€å‘ç¯å¢ƒè®¾ä¸ºFalseï¼Œç”Ÿäº§ç¯å¢ƒè®¾ä¸ºTrue
        SESSION_COOKIE_HTTPONLY=True,
        SESSION_COOKIE_SAMESITE='Lax',
        PERMANENT_SESSION_LIFETIME=timedelta(days=7)  # sessionæœ‰æ•ˆæœŸ7å¤©
    )
    
    # æ•°æ®åº“é…ç½®
    app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://root:123456@localhost:3306/eye_db'
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    
    # è®¾ç½®éšæœºå¯†é’¥
    app.secret_key = os.urandom(24)
    
    return app

# è·¯ç”±ï¼šæœåŠ¡é™æ€æ–‡ä»¶
@app.route('/')
def index():
    return send_from_directory(app.static_folder, 'index.html')

@app.route('/<path:path>')
def serve_static(path):
    return send_from_directory(app.static_folder, path)

def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return jsonify({"error": "Please login first"}), 401
        return f(*args, **kwargs)
    return decorated_function

# é¢„æµ‹æ¥å£
@app.route("/predict", methods=["POST"])
def predict():
    if "file" not in request.files:
        return jsonify({"error": "No file provided"}), 400

    file = request.files["file"]

    if not file.filename:
        return jsonify({"error": "No file selected"}), 400

    try:
        image = Image.open(file).convert("RGB")
        image_tensor = preprocess_image(image)

        with torch.no_grad():
            output = model(image_tensor)
            probs = torch.sigmoid(output).cpu().numpy().tolist()[0]

        classes = ["N", "D", "G", "C", "A", "H", "M", "O"]
        predictions = {cls: float(prob) for cls, prob in zip(classes, probs)}

        return jsonify({"predictions": predictions})
    except Exception as e:
        logger.error("Error during prediction: %s", str(e))
        return jsonify({"error": str(e)}), 500

@app.route("/logout", methods=["POST"])
def logout():
    session.clear()
    return jsonify({"success": True, "message": "ç™»å‡ºæˆåŠŸ"})

@app.route("/cases")
@login_required
def get_cases():
    page = request.args.get('page', 1, type=int)
    per_page = 10  # æ¯é¡µæ˜¾ç¤º10æ¡è®°å½•
    
    # è·å–å½“å‰ç”¨æˆ·çš„ç—…ä¾‹
    user_id = session.get('user_id')
    pagination = Case.query.filter_by(user_id=user_id)\
        .order_by(Case.created_at.desc())\
        .paginate(page=page, per_page=per_page, error_out=False)
    
    cases = []
    for case in pagination.items:
        case_data = {
            'id': case.id,
            'created_at': case.created_at.isoformat(),
            'description': case.description,
            'left_eye_image': case.left_eye_image,
            'right_eye_image': case.right_eye_image,
            'left_eye_result': case.left_eye_result,
            'right_eye_result': case.right_eye_result
        }
        cases.append(case_data)
    
    return jsonify({
        'cases': cases,
        'total_pages': pagination.pages,
        'current_page': page
    })

@app.route("/chat_ollama", methods=["POST"])
def chat_ollama():
    try:
        data = request.get_json()
        prompt = data.get("prompt")
        model_name = data.get("model", LLM_CONFIG["model_name"])

        if not prompt:
            return jsonify({"error": "No prompt provided"}), 400

        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: prompt={prompt}, model={model_name}")

        # æ„å»ºä¸Šä¸‹æ–‡
        context = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çœ¼ç§‘åŒ»ç”Ÿï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦æœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä¸è¦è¯´"æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIçœ¼ç§‘åŠ©æ‰‹"ï¼Œä¸è¦é‡å¤ç”¨æˆ·çš„é—®é¢˜ï¼Œç›´æ¥ç»™å‡ºä¸“ä¸šçš„å›ç­”ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š{prompt}"""

        try:
            # æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
            if LLM_CONFIG["model_type"] == "local":
                # é¦–å…ˆæµ‹è¯•OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
                health_check = requests.get("http://localhost:11434/api/tags", timeout=5)
                if health_check.status_code != 200:
                    logger.error("OllamaæœåŠ¡æœªè¿è¡Œæˆ–æ— å“åº”")
                    return jsonify({"error": "AIæœåŠ¡æœªå¯åŠ¨ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ"}), 503
                # è°ƒç”¨Ollama API
                logger.info(f"æ­£åœ¨è°ƒç”¨Ollama APIï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": context,
                        "stream": False,
                        "temperature": 0.7,
                        "top_p": 0.9
                    },
                    timeout=30
                )
                logger.info(f"Ollama APIå“åº”çŠ¶æ€ç : {response.status_code}")
                # if response.status_code == 200:
                #     result = response.json()
                #     logger.info("æˆåŠŸè·å–AIå“åº”")
                #     ai_response = result.get("response", "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
                # else:
                #     logger.error(f"Ollama APIé”™è¯¯å“åº”: {response.text}")
                #     return jsonify({"error": "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"}), 500
                try:
                    if response.status_code == 200:
                        result_json = response.json()
                        logger.info(f"æœ¬åœ°æ¨¡å‹å“åº”: {result_json}")
                        ai_response = result_json.get("response")
                        # æ£€æŸ¥è¿”å›ç»“æ„ï¼Œæ‰¾å‡ºå®é™…çš„å­—æ®µ
                        # if "response" in result_json:
                        #     ai_response = result_json["response"]
                        # else:
                        #     ai_response = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
                    else:
                        logger.error(f"Ollama APIé”™è¯¯å“åº”: {response.text}")
                        return jsonify({"error": "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"}), 500
                    return jsonify({"response": ai_response})
                except Exception as e:
                    logger.error(f"è§£ææœ¬åœ°æ¨¡å‹å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    return jsonify({"error": "è§£æå“åº”æ—¶å‡ºé”™"}), 500

            else:  # LLM_CONFIG["model_type"] == "api"
                # è°ƒç”¨å¤–éƒ¨API
                logger.info(f"æ­£åœ¨è°ƒç”¨å¤–éƒ¨API: {LLM_CONFIG['api_url']}")
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {LLM_CONFIG['api_key']}"
                }
                # response = requests.post(...)
                payload = {
                    "model": LLM_CONFIG["model_name"],
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çœ¼ç§‘åŒ»ç”Ÿ..."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 1000
                }
                response = requests.post(
                    f"{LLM_CONFIG['api_url'].rstrip('/')}/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {LLM_CONFIG['api_key']}",
                        "Content-Type": "application/json"
                    },
                    json=payload
                )
                ai_response = response.json()["choices"][0]["message"]["content"]

                # æ ¹æ®ä¸åŒçš„APIè°ƒæ•´è¯·æ±‚æ ¼å¼
                if "deepseek" in LLM_CONFIG["api_url"].lower():
                    # OpenAIæ ¼å¼
                    print(LLM_CONFIG["api_url"])
                    print(LLM_CONFIG["api_key"])
                    client = OpenAI(api_key="sk-aa19071022fe47f1aaef8a0215749ddc", base_url="https://api.deepseek.com/v1")

                    payload = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çœ¼ç§‘åŒ»ç”Ÿï¼Œè¯·ä½¿ç”¨ä¸­æ–‡è¯¦ç»†å›ç­”ç”¨æˆ·æå‡ºçš„çœ¼ç§‘ç›¸å…³é—®é¢˜ã€‚å›ç­”ä¸­åº”åŒ…æ‹¬ï¼šé—®é¢˜çš„åŒ»å­¦è§£é‡Šã€å¯èƒ½çš„ç—…å› ã€å»ºè®®çš„æ£€æŸ¥æ–¹æ³•ã€æ˜¯å¦éœ€è¦å°±è¯Šã€æ—¥å¸¸æ³¨æ„äº‹é¡¹ç­‰ã€‚å¦‚æœé—®é¢˜æ¨¡ç³Šï¼Œå¯æç¤ºç”¨æˆ·è¡¥å……ç—‡çŠ¶ä¿¡æ¯ã€‚ç¦æ­¢è‡ªæˆ‘ä»‹ç»å’Œé‡å¤ç”¨æˆ·æé—®ã€‚"},
                            {"role": "user", "content": prompt},
                        ],
                        stream=False
                    )
                    ai_response = payload.choices[0].message.content
                    return jsonify({"response": ai_response})
                else:
                    # é€šç”¨æ ¼å¼
                    payload = {
                        "model": LLM_CONFIG["model_name"],
                        "prompt": context,
                        "temperature": 0.7,
                        "max_tokens": 1000
                    }
                response = requests.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {LLM_CONFIG['api_key']}",
                        "Content-Type": "application/json"
                    },
                    json=payload
                )
            # è¿‡æ»¤æ‰æ€è€ƒè¿‡ç¨‹å’Œå›ºå®šå¼€å¤´ç»“å°¾
            ai_response = re.sub(r'<think>[\s\S]*?</think>', '', ai_response)
            ai_response = re.sub(r'æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIçœ¼ç§‘åŠ©æ‰‹ã€‚æ‚¨çš„é—®é¢˜æ˜¯ï¼š.*?ã€‚', '', ai_response)
            ai_response = re.sub(r'æˆ‘æ­£åœ¨ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„çœ¼ç§‘å’¨è¯¢æœåŠ¡ã€‚', '', ai_response)
            # ai_response = response.choices[0].message.content
            # è¿œç¨‹apiè°ƒç”¨çš„è¾“å‡ºæ ¼å¼
            ai_response = response.json()["choices"][0]["message"]["content"]
            # å¦‚æœè¿‡æ»¤åå†…å®¹ä¸ºç©ºï¼Œæä¾›é»˜è®¤å›å¤

            if not ai_response.strip():
                ai_response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„ç—‡çŠ¶ã€‚"
            logger.info(f"è¿‡æ»¤åçš„AIå“åº”å†…å®¹: {ai_response}")
            return jsonify({"response": ai_response})

        except requests.exceptions.ConnectionError:
            logger.error("æ— æ³•è¿æ¥åˆ°AIæœåŠ¡")
            return jsonify({"error": "æ— æ³•è¿æ¥åˆ°AIæœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ"}), 503
        except requests.exceptions.Timeout:
            logger.error("è¯·æ±‚AIæœåŠ¡è¶…æ—¶")
            return jsonify({"error": "AIæœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"}), 504
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚AIæœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return jsonify({"error": "ä¸AIæœåŠ¡é€šä¿¡æ—¶å‘ç”Ÿé”™è¯¯"}), 500

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({"error": "Not found"}), 404

@app.errorhandler(500)
def server_error(error):
    return jsonify({"error": "Internal server error"}), 500

# æ·»åŠ è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨çš„å‡½æ•°
def open_browser():
    time.sleep(1)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    webbrowser.open('http://127.0.0.1:5001')

@app.route("/save_detection", methods=["POST"])
@login_required
def save_detection():
    try:
        data = request.get_json()
        print("Received data for saving:", {
            'description': data.get('description'),
            'has_left_eye': 'left_eye' in data,
            'has_right_eye': 'right_eye' in data
        })
        
        # è·å–å½“å‰ç”¨æˆ·ID
        user_id = session.get('user_id')
        print(f"Current session data: {dict(session)}")  # è°ƒè¯•ä¿¡æ¯
        
        if not user_id:
            print("No user_id found in session")  # è°ƒè¯•ä¿¡æ¯
            return jsonify({"error": "User not found"}), 401

        # ä¿å­˜å·¦çœ¼å›¾ç‰‡
        left_eye_image = data.get('left_eye', {}).get('image')
        left_eye_path = None
        if left_eye_image:
            print("Saving left eye image...")  # è°ƒè¯•ä¿¡æ¯
            saved_path = save_base64_image(left_eye_image, UPLOAD_FOLDER)
            if saved_path:
                # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                left_eye_path = os.path.relpath(saved_path, 'ai_detection')
                print(f"Left eye image saved to: {left_eye_path}")  # è°ƒè¯•ä¿¡æ¯
            else:
                print("Failed to save left eye image")  # è°ƒè¯•ä¿¡æ¯

        # ä¿å­˜å³çœ¼å›¾ç‰‡
        right_eye_image = data.get('right_eye', {}).get('image')
        right_eye_path = None
        if right_eye_image:
            print("Saving right eye image...")  # è°ƒè¯•ä¿¡æ¯
            saved_path = save_base64_image(right_eye_image, UPLOAD_FOLDER)
            if saved_path:
                # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                right_eye_path = os.path.relpath(saved_path, 'ai_detection')
                print(f"Right eye image saved to: {right_eye_path}")  # è°ƒè¯•ä¿¡æ¯
            else:
                print("Failed to save right eye image")  # è°ƒè¯•ä¿¡æ¯

        print(f"Creating new case for user {user_id}")  # è°ƒè¯•ä¿¡æ¯

        # åˆ›å»ºæ–°çš„ç—…ä¾‹è®°å½•
        new_case = Case(
            user_id=user_id,
            description=data.get('description', ''),
            left_eye_image=left_eye_path,
            right_eye_image=right_eye_path,
            left_eye_result=json.dumps(data.get('left_eye', {}).get('results', {})),
            right_eye_result=json.dumps(data.get('right_eye', {}).get('results', {})),
            created_at=datetime.datetime.utcnow()
        )

        # ä¿å­˜åˆ°æ•°æ®åº“
        try:
            db.session.add(new_case)
            db.session.commit()
            print(f"Successfully saved case with ID: {new_case.id}")  # è°ƒè¯•ä¿¡æ¯
        except Exception as db_error:
            print(f"Database error: {str(db_error)}")  # è°ƒè¯•ä¿¡æ¯
            db.session.rollback()
            raise

        return jsonify({
            "success": True,
            "message": "æ£€æµ‹ç»“æœå·²ä¿å­˜",
            "case_id": new_case.id
        })

    except Exception as e:
        db.session.rollback()
        print(f"Error saving detection: {str(e)}")  # è°ƒè¯•ä¿¡æ¯
        return jsonify({"error": str(e)}), 500

# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡è·¯ç”±
@app.route('/staticImage/<path:filename>')
def serve_static_image(filename):
    return send_from_directory(UPLOAD_FOLDER, filename)

@app.route("/case/<int:case_id>")
@login_required
def get_case(case_id):
    try:
        # è·å–å½“å‰ç”¨æˆ·çš„ç—…ä¾‹
        user_id = session.get('user_id')
        case = Case.query.filter_by(id=case_id, user_id=user_id).first()
        
        if not case:
            return jsonify({"error": "ç—…ä¾‹ä¸å­˜åœ¨æˆ–æ— æƒè®¿é—®"}), 404

        return jsonify({
            "id": case.id,
            "created_at": case.created_at.isoformat(),
            "description": case.description,
            "left_eye_image": case.left_eye_image,
            "right_eye_image": case.right_eye_image,
            "left_eye_result": case.left_eye_result,
            "right_eye_result": case.right_eye_result
        })
    except Exception as e:
        print(f"Error getting case: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # é…ç½®åº”ç”¨
    app = configure_app(app)
    
    # è®¾ç½®å¤§è¯­è¨€æ¨¡å‹é…ç½®
    setup_llm_config()
    
    # åŠ è½½çœ¼éƒ¨ç–¾ç—…é¢„æµ‹æ¨¡å‹
    model = load_model(MODEL_PATH)
    if model is None:
        print("Warning: Running without model. Predictions will not work!")

    with app.app_context():
        print("ğŸ§± æ­£åœ¨å»ºè¡¨åˆ° eye_db...")
        db.create_all()
        print("âœ… æ•°æ®åº“è¡¨ç»“æ„å·²åˆå§‹åŒ–å®Œæ¯•")

    # åœ¨æ–°çº¿ç¨‹ä¸­æ‰“å¼€æµè§ˆå™¨
    threading.Thread(target=open_browser).start()

    logger.info("Server started")
    # å¯åŠ¨æœåŠ¡å™¨
    app.run(host="0.0.0.0", port=5001, debug=True, use_reloader=False) 
